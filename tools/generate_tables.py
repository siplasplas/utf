#!/usr/bin/env python3
"""
Generator tablic Unicode dla biblioteki UTF.
Generuje pliki C++ z tablicami case mapping i decomposition.

Użycie:
    python generate_tables.py

Wymaga wcześniejszego uruchomienia get.sh
"""

import os
import sys
from pathlib import Path

SCRIPT_DIR = Path(__file__).parent
DATA_DIR = SCRIPT_DIR / "unicode_data"
OUTPUT_DIR = SCRIPT_DIR.parent / "generated"


def parse_unicode_data(filepath):
    """
    Parsuje UnicodeData.txt
    Zwraca słownik: code_point -> {name, category, upper, lower, title, decomp}
    """
    data = {}
    with open(filepath, 'r', encoding='utf-8') as f:
        for line in f:
            line = line.strip()
            if not line:
                continue
            fields = line.split(';')
            if len(fields) < 15:
                continue

            cp = int(fields[0], 16)
            name = fields[1]
            category = fields[2]
            decomp = fields[5]  # Decomposition mapping
            upper = int(fields[12], 16) if fields[12] else 0
            lower = int(fields[13], 16) if fields[13] else 0
            title = int(fields[14], 16) if fields[14] else 0

            data[cp] = {
                'name': name,
                'category': category,
                'upper': upper,
                'lower': lower,
                'title': title,
                'decomp': decomp
            }
    return data


def parse_special_casing(filepath):
    """
    Parsuje SpecialCasing.txt
    Zwraca listę: [(code_point, lower_list, title_list, upper_list, condition)]
    """
    special = []
    with open(filepath, 'r', encoding='utf-8') as f:
        for line in f:
            line = line.split('#')[0].strip()
            if not line:
                continue
            fields = [f.strip() for f in line.split(';')]
            if len(fields) < 5:
                continue

            cp = int(fields[0], 16)
            lower = [int(x, 16) for x in fields[1].split() if x]
            title = [int(x, 16) for x in fields[2].split() if x]
            upper = [int(x, 16) for x in fields[3].split() if x]
            condition = fields[4] if len(fields) > 4 else ""

            # Pomijamy warunkowe mappingi (np. Final_Sigma)
            if not condition:
                special.append((cp, lower, title, upper))

    return special


def generate_case_data(unicode_data, special_casing):
    """
    Generuje plik CaseData.cpp z tablicami case mapping.
    """
    # Simple case mappings (1:1)
    upper_map = []
    lower_map = []

    for cp, info in sorted(unicode_data.items()):
        if info['upper'] and info['upper'] != cp:
            upper_map.append((cp, info['upper']))
        if info['lower'] and info['lower'] != cp:
            lower_map.append((cp, info['lower']))

    # Special case mappings (1:N) - tylko bezwarunkowe
    special_upper = []
    special_lower = []

    for cp, lower, title, upper in special_casing:
        if len(upper) > 1:
            special_upper.append((cp, upper))
        if len(lower) > 1:
            special_lower.append((cp, lower))

    # Sort for binary search
    special_upper.sort(key=lambda x: x[0])
    special_lower.sort(key=lambda x: x[0])

    # Generuj C++
    output = []
    output.append("// Auto-generated by generate_tables.py")
    output.append("// Do not edit manually!")
    output.append("")
    output.append('#include "utf/UnicodeData.hpp"')
    output.append("")
    output.append("namespace utf::data {")
    output.append("")

    # toLower mapping (uppercase -> lowercase)
    output.append("// Simple toLower mapping: uppercase code point -> lowercase code point")
    output.append(f"const std::pair<char32_t, char32_t> lower_map[] = {{")
    for cp, target in lower_map:
        output.append(f"    {{0x{cp:04X}, 0x{target:04X}}},")
    output.append("};")
    output.append(f"const size_t lower_map_size = {len(lower_map)};")
    output.append("")

    # toUpper mapping (lowercase -> uppercase)
    output.append("// Simple toUpper mapping: lowercase code point -> uppercase code point")
    output.append(f"const std::pair<char32_t, char32_t> upper_map[] = {{")
    for cp, target in upper_map:
        output.append(f"    {{0x{cp:04X}, 0x{target:04X}}},")
    output.append("};")
    output.append(f"const size_t upper_map_size = {len(upper_map)};")
    output.append("")

    # Special upper (1:N)
    output.append("// Special toUpper mapping (1:N), e.g., ß -> SS")
    output.append("const SpecialCase special_upper[] = {")
    for cp, targets in special_upper:
        targets_padded = targets + [0] * (3 - len(targets))
        targets_str = ", ".join(f"0x{t:04X}" for t in targets_padded)
        output.append(f"    {{0x{cp:04X}, {{{targets_str}}}, {len(targets)}}},")
    output.append("};")
    output.append(f"const size_t special_upper_size = {len(special_upper)};")
    output.append("")

    # Special lower (1:N) - rzadkie, ale dla kompletności
    output.append("const SpecialCase special_lower[] = {")
    for cp, targets in special_lower:
        targets_padded = targets + [0] * (3 - len(targets))
        targets_str = ", ".join(f"0x{t:04X}" for t in targets_padded)
        output.append(f"    {{0x{cp:04X}, {{{targets_str}}}, {len(targets)}}},")
    if not special_lower:
        output.append("    {0, {0, 0, 0}, 0},  // placeholder")
    output.append("};")
    output.append(f"const size_t special_lower_size = {len(special_lower)};")
    output.append("")

    output.append("} // namespace utf::data")
    output.append("")

    return "\n".join(output)


def generate_decomp_data(unicode_data):
    """
    Generuje plik DecompData.cpp z tablicami decomposition.
    """
    decomp_map = []

    for cp, info in sorted(unicode_data.items()):
        decomp = info['decomp']
        if not decomp:
            continue

        # Pomijamy compatibility decomposition (zaczyna się od <tag>)
        if decomp.startswith('<'):
            continue

        # Canonical decomposition
        parts = decomp.split()
        if len(parts) >= 1:
            base = int(parts[0], 16)
            combining = int(parts[1], 16) if len(parts) > 1 else 0
            decomp_map.append((cp, base, combining))

    # Generuj C++
    output = []
    output.append("// Auto-generated by generate_tables.py")
    output.append("// Do not edit manually!")
    output.append("")
    output.append('#include "utf/UnicodeData.hpp"')
    output.append("")
    output.append("namespace utf::data {")
    output.append("")
    output.append("// Canonical decomposition for accent folding")
    output.append("const Decomposition decomp_map[] = {")
    for cp, base, combining in decomp_map:
        output.append(f"    {{0x{cp:04X}, 0x{base:04X}, 0x{combining:04X}}},")
    output.append("};")
    output.append(f"const size_t decomp_map_size = {len(decomp_map)};")
    output.append("")

    # Aggressive folding (ł->l, ø->o, etc.)
    aggressive = [
        (0x0141, 0x004C),  # Ł -> L
        (0x0142, 0x006C),  # ł -> l
        (0x00D8, 0x004F),  # Ø -> O
        (0x00F8, 0x006F),  # ø -> o
        (0x0110, 0x0044),  # Đ -> D
        (0x0111, 0x0064),  # đ -> d
        (0x0126, 0x0048),  # Ħ -> H
        (0x0127, 0x0068),  # ħ -> h
        (0x0166, 0x0054),  # Ŧ -> T
        (0x0167, 0x0074),  # ŧ -> t
        (0x014A, 0x004E),  # Ŋ -> N
        (0x014B, 0x006E),  # ŋ -> n
    ]
    aggressive.sort(key=lambda x: x[0])  # Sort for binary search

    output.append("// Aggressive folding for search (letters without canonical decomposition)")
    output.append("const std::pair<char32_t, char32_t> aggressive_fold[] = {")
    for cp, base in aggressive:
        output.append(f"    {{0x{cp:04X}, 0x{base:04X}}},")
    output.append("};")
    output.append(f"const size_t aggressive_fold_size = {len(aggressive)};")
    output.append("")

    # Aggressive expand (ß->ss, æ->ae, etc.)
    aggressive_expand = [
        (0x00DF, "ss"),   # ß -> ss
        (0x00E6, "ae"),   # æ -> ae
        (0x00C6, "AE"),   # Æ -> AE
        (0x0153, "oe"),   # œ -> oe
        (0x0152, "OE"),   # Œ -> OE
        (0x00F0, "d"),    # ð -> d
        (0x00D0, "D"),    # Ð -> D
        (0x00FE, "th"),   # þ -> th
        (0x00DE, "TH"),   # Þ -> TH
    ]
    aggressive_expand.sort(key=lambda x: x[0])  # Sort for binary search

    output.append("// Aggressive expand for search (1:N mappings)")
    output.append("const AggressiveExpand aggressive_expand[] = {")
    for cp, to in aggressive_expand:
        output.append(f'    {{0x{cp:04X}, "{to}"}},')
    output.append("};")
    output.append(f"const size_t aggressive_expand_size = {len(aggressive_expand)};")
    output.append("")

    output.append("} // namespace utf::data")
    output.append("")

    return "\n".join(output)


def main():
    # Sprawdź czy dane istnieją
    unicode_data_file = DATA_DIR / "UnicodeData.txt"
    special_casing_file = DATA_DIR / "SpecialCasing.txt"

    if not unicode_data_file.exists():
        print(f"Error: {unicode_data_file} not found. Run get.sh first.")
        sys.exit(1)

    if not special_casing_file.exists():
        print(f"Error: {special_casing_file} not found. Run get.sh first.")
        sys.exit(1)

    # Parsuj dane
    print("Parsing UnicodeData.txt...")
    unicode_data = parse_unicode_data(unicode_data_file)
    print(f"  Found {len(unicode_data)} code points")

    print("Parsing SpecialCasing.txt...")
    special_casing = parse_special_casing(special_casing_file)
    print(f"  Found {len(special_casing)} special casing entries")

    # Utwórz katalog wyjściowy
    OUTPUT_DIR.mkdir(exist_ok=True)

    # Generuj pliki
    print("Generating CaseData.cpp...")
    case_data = generate_case_data(unicode_data, special_casing)
    case_file = OUTPUT_DIR / "CaseData.cpp"
    with open(case_file, 'w', encoding='utf-8') as f:
        f.write(case_data)
    print(f"  Written to {case_file}")

    print("Generating DecompData.cpp...")
    decomp_data = generate_decomp_data(unicode_data)
    decomp_file = OUTPUT_DIR / "DecompData.cpp"
    with open(decomp_file, 'w', encoding='utf-8') as f:
        f.write(decomp_data)
    print(f"  Written to {decomp_file}")

    print("Done!")


if __name__ == "__main__":
    main()
