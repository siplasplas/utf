#!/usr/bin/env python3
"""
Generator tablic collation dla biblioteki UTF.
Parsuje pliki CLDR collation i generuje tablice wag sortowania.

Format CLDR tailoring:
  &A<ą<<<Ą  = po A wstaw ą (primary), Ą to tertiary różnica od ą
  < = primary (różne litery)
  << = secondary (akcenty)
  <<< = tertiary (wielkość liter)
"""

import os
import re
import xml.etree.ElementTree as ET
from pathlib import Path
import unicodedata

SCRIPT_DIR = Path(__file__).parent
DATA_DIR = SCRIPT_DIR / "unicode_data" / "cldr" / "collation"
OUTPUT_DIR = SCRIPT_DIR.parent / "generated"


def parse_cldr_rules(xml_path):
    """
    Parsuje plik CLDR XML i zwraca reguły tailoringu.
    Zwraca: [(anchor_char, [(char_or_str, level), ...]), ...]
    level: 1=primary, 2=secondary, 3=tertiary
    """
    try:
        tree = ET.parse(xml_path)
        root = tree.getroot()
    except Exception as e:
        print(f"  Warning: Cannot parse {xml_path}: {e}")
        return []

    rules = []

    # Find collation rules
    for collation in root.findall('.//collation[@type="standard"]'):
        cr = collation.find('cr')
        if cr is None or cr.text is None:
            continue

        text = cr.text.strip()
        # Parse rules like: &A<ą<<<Ą
        # Split by & to get each anchor group
        parts = text.split('&')
        for part in parts:
            part = part.strip()
            if not part:
                continue

            # Parse: anchor followed by tailorings
            # E.g., "A<ą<<<Ą" or "H<ch<<<cH<<<Ch<<<CH"
            tailorings = []
            current_pos = 0
            anchor = None

            # Find anchor (first character(s) before any < or =)
            match = re.match(r'^([^<>=]+)', part)
            if match:
                anchor = match.group(1).strip()
                current_pos = match.end()

            if not anchor:
                continue

            # Parse the rest: <char or <<char or <<<char
            remaining = part[current_pos:]
            # Pattern: operator followed by characters until next operator
            pattern = r'(<<<|<<|<|=)([^<>=]+)'
            for m in re.finditer(pattern, remaining):
                op = m.group(1)
                chars = m.group(2).strip()
                if not chars:
                    continue

                if op == '<':
                    level = 1  # primary
                elif op == '<<':
                    level = 2  # secondary
                elif op == '<<<' or op == '=':
                    level = 3  # tertiary
                else:
                    continue

                tailorings.append((chars, level))

            if tailorings:
                rules.append((anchor, tailorings))

    return rules


def build_collation_weights(rules):
    """
    Buduje tablicę wag na podstawie reguł tailoringu.
    Zwraca: {char_or_str: (primary, secondary, tertiary)}
    """
    weights = {}

    # Start with base weights for ASCII
    # A-Z get primary weights 100-350 (with gaps for tailoring)
    for i, c in enumerate('ABCDEFGHIJKLMNOPQRSTUVWXYZ'):
        weights[c] = (100 + i * 10, 0, 0)  # uppercase
        weights[c.lower()] = (100 + i * 10, 0, 1)  # lowercase (tertiary diff)

    # Apply tailoring rules
    for anchor, tailorings in rules:
        # Find anchor weight
        anchor_upper = anchor.upper()
        if anchor in weights:
            base_primary = weights[anchor][0]
        elif anchor_upper in weights:
            base_primary = weights[anchor_upper][0]
        else:
            # Unknown anchor - try to find by first char
            first_char = anchor[0].upper()
            if first_char in weights:
                base_primary = weights[first_char][0]
            else:
                continue

        # Assign weights to tailored characters
        sub_offset = 1

        for chars, level in tailorings:
            if level == 1:
                # Primary difference - increment offset
                sub_offset += 1
                secondary = 0
                tertiary = 0
            elif level == 2:
                # Secondary difference
                secondary = sub_offset
                tertiary = 0
            else:
                # Tertiary difference (case)
                secondary = 0
                tertiary = sub_offset

            # Store weight - normalize to NFC for consistent lookup
            weight = (base_primary + sub_offset, secondary, tertiary)
            normalized_chars = unicodedata.normalize('NFC', chars)
            weights[normalized_chars] = weight

    return weights


def generate_collation_data(locales_data):
    """
    Generuje plik CollationData.cpp z tablicami collation.
    """
    output = []
    output.append("// Auto-generated by generate_collation.py")
    output.append("// Do not edit manually!")
    output.append("")
    output.append('#include "utf/Collator.hpp"')
    output.append("")
    output.append("namespace utf::data {")
    output.append("")

    # Generate weight tables for each locale
    for locale, weights in locales_data.items():
        # Sort by character/string for deterministic output
        sorted_weights = sorted(weights.items(), key=lambda x: (len(x[0]), x[0]))

        # Single char entries
        single_chars = [(k, v) for k, v in sorted_weights if len(k) == 1]
        # Multi-char entries (like "ch")
        multi_chars = [(k, v) for k, v in sorted_weights if len(k) > 1]

        output.append(f"// Locale: {locale}")
        output.append(f"const CollationWeight collation_{locale}_single[] = {{")
        for char, (p, s, t) in single_chars:
            cp = ord(char)
            output.append(f"    {{0x{cp:04X}, {p}, {s}, {t}}},  // {repr(char)}")
        output.append("};")
        output.append(f"const size_t collation_{locale}_single_size = {len(single_chars)};")
        output.append("")

        if multi_chars:
            output.append(f"const CollationContraction collation_{locale}_multi[] = {{")
            for chars, (p, s, t) in multi_chars:
                # Store as UTF-8 string
                output.append(f'    {{"{chars}", {p}, {s}, {t}}},')
            output.append("};")
            output.append(f"const size_t collation_{locale}_multi_size = {len(multi_chars)};")
        else:
            output.append(f"const CollationContraction* collation_{locale}_multi = nullptr;")
            output.append(f"const size_t collation_{locale}_multi_size = 0;")
        output.append("")

    # Generate locale table
    output.append("// Available locales")
    output.append("const LocaleCollation locale_collations[] = {")
    for locale in locales_data.keys():
        output.append(f'    {{"{locale}", collation_{locale}_single, collation_{locale}_single_size, collation_{locale}_multi, collation_{locale}_multi_size}},')
    output.append("};")
    output.append(f"const size_t locale_collations_size = {len(locales_data)};")
    output.append("")

    output.append("} // namespace utf::data")
    output.append("")

    return "\n".join(output)


def main():
    if not DATA_DIR.exists():
        print(f"Error: {DATA_DIR} not found. Run get.sh first.")
        return

    # Parse locales
    locales_data = {}

    # Always include "root" with basic Latin ordering
    print("Building root collation...")
    root_weights = {}
    for i, c in enumerate('ABCDEFGHIJKLMNOPQRSTUVWXYZ'):
        root_weights[c] = (100 + i * 10, 0, 0)
        root_weights[c.lower()] = (100 + i * 10, 0, 1)
    locales_data['root'] = root_weights

    # Parse locale files
    locale_files = ['pl', 'cs', 'de', 'ru', 'uk']

    for locale in locale_files:
        xml_path = DATA_DIR / f"{locale}.xml"
        if not xml_path.exists():
            print(f"  Skipping {locale}: file not found")
            continue

        print(f"Parsing {locale}.xml...")
        rules = parse_cldr_rules(xml_path)
        if rules:
            weights = build_collation_weights(rules)
            locales_data[locale] = weights
            print(f"  Found {len(weights)} weight entries")
        else:
            print(f"  No rules found, using root")
            locales_data[locale] = root_weights.copy()

    # Generate C++
    OUTPUT_DIR.mkdir(exist_ok=True)

    print("Generating CollationData.cpp...")
    collation_data = generate_collation_data(locales_data)
    collation_file = OUTPUT_DIR / "CollationData.cpp"
    with open(collation_file, 'w', encoding='utf-8') as f:
        f.write(collation_data)
    print(f"  Written to {collation_file}")

    print("Done!")


if __name__ == "__main__":
    main()
